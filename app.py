"""
============================================
FIRE & SMOKE DETECTION API
============================================
Render + TensorFlow + Supabase (FIXED)
============================================
"""

from flask import Flask, request, jsonify
import tensorflow as tf
import numpy as np
from PIL import Image
import io
import base64
import os
from datetime import datetime

# ============================================
# SUPABASE INIT (FIXED VERSION)
# ============================================

supabase = None
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

if SUPABASE_URL and SUPABASE_KEY:
    try:
        from supabase import create_client
        # Fix: Kh√¥ng truy·ªÅn proxy parameter
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        print("‚úÖ Supabase connected successfully")
    except Exception as e:
        print(f"‚ùå Supabase init failed: {e}")
        supabase = None

# ============================================
# FLASK APP
# ============================================

app = Flask(__name__)
app.config["MAX_CONTENT_LENGTH"] = 16 * 1024 * 1024  # 16MB max

# ============================================
# MODEL LOADING
# ============================================

MODEL_PATH = "fire_smoke_detection_model"
model = None

def load_model():
    """Load TensorFlow model once"""
    global model
    if model is None:
        try:
            print("üî• Loading model...")
            model = tf.keras.models.load_model(MODEL_PATH)
            print("‚úÖ Model loaded successfully")
        except Exception as e:
            print(f"‚ùå Model loading failed: {e}")
            raise
    return model

# ============================================
# IMAGE PREPROCESSING
# ============================================

def preprocess_image(base64_image):
    """
    Chuy·ªÉn ƒë·ªïi base64 image th√†nh tensor cho model
    Input: base64 string (kh√¥ng c√≥ prefix)
    Output: numpy array (1, 224, 224, 3)
    """
    try:
        # Decode base64
        img_data = base64.b64decode(base64_image)
        img = Image.open(io.BytesIO(img_data))
        
        # Convert to RGB v√† resize
        img = img.convert("RGB").resize((224, 224))
        
        # Convert to array v√† normalize
        arr = np.asarray(img, dtype=np.float32) / 255.0
        
        # Add batch dimension
        return np.expand_dims(arr, axis=0)
    except Exception as e:
        raise ValueError(f"Image preprocessing failed: {e}")

# ============================================
# API ROUTES
# ============================================

@app.route("/")
def index():
    """Health check endpoint"""
    return jsonify({
        "status": "online",
        "service": "Fire & Smoke Detection API",
        "model_loaded": model is not None,
        "supabase": "connected" if supabase else "disabled",
        "endpoints": {
            "predict": "/predict (POST)",
            "health": "/ (GET)"
        }
    })

@app.route("/health")
def health():
    """Detailed health check"""
    return jsonify({
        "status": "healthy",
        "model": "loaded" if model else "not loaded",
        "supabase": "connected" if supabase else "disabled",
        "timestamp": datetime.utcnow().isoformat()
    })

@app.route("/predict", methods=["POST"])
def predict():
    """
    Main prediction endpoint
    
    Request body (JSON):
    {
        "image": "base64_encoded_image_string"
    }
    
    Response:
    {
        "class": "Fire|Neutral|Smoke",
        "confidence": 95.23,
        "timestamp": "2026-01-03T10:30:00.000000",
        "probabilities": {
            "Fire": 95.23,
            "Neutral": 2.45,
            "Smoke": 2.32
        }
    }
    """
    try:
        # Validate request
        data = request.get_json()
        if not data or "image" not in data:
            return jsonify({
                "error": "Missing 'image' field in request body",
                "example": {"image": "base64_string_here"}
            }), 400
        
        # Load model n·∫øu ch∆∞a load
        mdl = load_model()
        
        # Preprocess image
        img = preprocess_image(data["image"])
        
        # Predict
        preds = mdl.predict(img, verbose=0)[0]
        
        # Class labels
        labels = ["Fire", "Neutral", "Smoke"]
        idx = int(np.argmax(preds))
        
        # Prepare result
        result = {
            "class": labels[idx],
            "confidence": round(float(preds[idx]) * 100, 2),
            "timestamp": datetime.utcnow().isoformat(),
            "probabilities": {
                labels[i]: round(float(preds[i]) * 100, 2) 
                for i in range(len(labels))
            }
        }
        
        # Save to Supabase (n·∫øu c√≥)
        if supabase:
            try:
                supabase.table("predictions").insert({
                    "class": result["class"],
                    "confidence": result["confidence"],
                    "timestamp": result["timestamp"],
                    "fire_prob": result["probabilities"]["Fire"],
                    "neutral_prob": result["probabilities"]["Neutral"],
                    "smoke_prob": result["probabilities"]["Smoke"]
                }).execute()
                result["saved_to_db"] = True
            except Exception as db_error:
                print(f"‚ö†Ô∏è Database save failed: {db_error}")
                result["saved_to_db"] = False
        
        return jsonify(result)
        
    except ValueError as ve:
        return jsonify({"error": str(ve)}), 400
    except Exception as e:
        print(f"‚ùå Prediction error: {e}")
        return jsonify({
            "error": "Internal server error",
            "message": str(e)
        }), 500

# ============================================
# ERROR HANDLERS
# ============================================

@app.errorhandler(413)
def request_entity_too_large(error):
    return jsonify({
        "error": "File too large",
        "max_size": "16MB"
    }), 413

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        "error": "Endpoint not found",
        "available_endpoints": ["/", "/health", "/predict"]
    }), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        "error": "Internal server error"
    }), 500

# ============================================
# MAIN
# ============================================

if __name__ == "__main__":
    # Pre-load model khi start
    try:
        load_model()
        print("üöÄ Server starting...")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Could not pre-load model: {e}")
    
    # Run Flask app
    port = int(os.getenv("PORT", 5000))
    app.run(
        host="0.0.0.0",
        port=port,
        debug=False
    )